# Задача №3. Обучение тематической модели с помощью PySpark

### Постановка задачи: обучить тематическую модель PLSA.

В Jupyter Notebook нужно описать и протестировать EM-алгоритм для обучения тематической модели PLSA в PySpark. Нужно реализовать оффлайновый синхронный алгоритм без хранения $\Theta$ и $n_{td}$ с несколькими проходами на E-шаге по каждому документу. Алгоритм должен быть реализован в парадигме MapReduce (E-шаг выполняется на воркерах, M-шаг - на драйвере).

Листинг алгоритма представлен на слайдах 83 (без сохранения и возвращения $\theta_d$) и 84 (без общей формулы регуляризации на M-шаге, только нормировка $n_{wt}$).

Реализация должна иметь интерфейс, поддерживающий следующий сценарий использования:

```
model = TopicModel(num_topics=20,              # число тем в модели
                   cv_model=cv_model,          # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`
                   nnz=nnz,                    # общее число словопозиций в коллекции
                   num_document_passes=5,      # число проходов по документу на E-шаге
                   use_phi_broadcast=True,     # использование бродкастинга матрицы $\Phi$
                   beta=0.0)                   # коэффициент регуляризации

model.fit(bow_data,                            # данные, объект класса `pyspark.rdd.PipelinedRDD` (содержит `pyspark.mllib.linalg.SparseVector`)
          num_collection_passes=10)            # число проходов по коллекции

model.print_perplexity()
model.print_topics(num_tokens=10)              # количество выводимых топ-слов
```

### Задание:

1. Описать класс `TopicModel`, реализующий EM-алгоритм в парадигме MapReduce:
- на шаге Map выполнять обрабатывать за раз целый partition, генерировать на выходе `numpy`-массив размера $|W|\times|T|$ счетчиков $n_{wt}$ и значение лог-правдоподобия, максимально использовать векторизацию операций; +
- на шаге Reduce агрегировать полученные матрицы и значения лог-правдоподобия; +
- на M-шаге нормировать счетчики и сохранять новую версию матрицы $\Phi$; +
- реализовать возможность использования $\Phi$ в виде broadcast-переменной; +
- обучение должно производиться с помощью метода `fit`; +
- реализовать метод `print_perplexity`, печатающий список значений перплексии на конец каждой итерации обработки коллекции; + 
- реализовать метод `print_topics`, печатающий список из `num_tokens` наиболее вероятных слов в каждой теме; +
2. Провести ряд экспериментов с реализованной моделью на данных `'vw.wiki-en-20K.txt'`:
- обучить модель с числом тем `num_topics` = 10, 20, 50, оценить время работы при `num_document_passes=5` и `num_collection_passes=10` в случае использования бродкастинга и без него, сделать выводы; перед запуском обучения нужно убедиться, что данные разбиты на достаточное число партиций и что среди них нет вырожденных (существенно меньших по объёму, чем прочие), информацию о разбиении нужно привести в отчете;
- при `num_topics=20` и `num_collection_passes=10` попробовать различные значения `num_document_passes` = 1, 2, 5, 10, оценить скорость работы и сравнить значения перплексии и интерпретируемость тем по топ-словам, сделать выводы;
3. Добавить в EM-алгоритм простейшую регуляризацию а-ля LDA (сглаживание/разреживание константой `beta`):
- попробовать разреживание с `beta`= 0.0, -0.1, -1.0 при `num_topics=20`, `num_collection_passes=10` и `num_document_passes=5`, сравнить значения перплексии и разреженность итоговой матрицы $\Phi$, сделать выводы;
- подобрать коэффициент `beta`, дающий разреженность итоговой $\Phi$ выше 95 процентов;
- заменить в реализации структуру для хранения счётчиков $n_{wt}$ и матрицы $\Phi$ с `numpy`-массива на любую другую из `scipy` или библиотеки Spark, подходяющую для хранения разреженных матриц;
- повторить эксперимент из п. 2.1, сравнить скорость работы с плотным вариантом хранения при использовании сильного разреживания (> 95 процентов).

 Все результаты экспериментов должны оформляться в виде аккуратных таблиц со значениями и списков с выводами, все должно читаться и пониматься легко без кода.