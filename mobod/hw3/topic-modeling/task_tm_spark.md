# Задача №3. Обучение тематической модели с помощью PySpark

### Сроки сдачи
Hard deadline: 29.12.2021, 23:59.


### Постановка задачи: обучить тематическую модель PLSA.

В Jupyter Notebook нужно описать и протестировать EM-алгоритм для обучения тематической модели PLSA в PySpark. Нужно реализовать оффлайновый синхронный алгоритм без хранения $\Theta$ и $n_{td}$ с несколькими проходами на E-шаге по каждому документу. Алгоритм должен быть реализован в парадигме MapReduce (E-шаг выполняется на воркерах, M-шаг - на драйвере).

Листинг алгоритма представлен на слайдах 83 (без сохранения и возвращения $\theta_d$) и 84 (без общей формулы регуляризации на M-шаге, только нормировка $n_{wt}$).

Реализация должна иметь интерфейс, поддерживающий следующий сценарий использования:

```
model = TopicModel(num_topics=20,              # число тем в модели
                   cv_model=cv_model,          # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`
                   nnz=nnz,                    # общее число словопозиций в коллекции
                   num_document_passes=5,      # число проходов по документу на E-шаге
                   use_phi_broadcast=True,     # использование бродкастинга матрицы $\Phi$
                   beta=0.0)                   # коэффициент регуляризации

model.fit(bow_data,                            # данные, объект класса `pyspark.rdd.PipelinedRDD` (содержит `pyspark.mllib.linalg.SparseVector`)
          num_collection_passes=10)            # число проходов по коллекции

model.print_perplexity()
model.print_topics(num_tokens=10)              # количество выводимых топ-слов
```

### Задание:

1. Описать класс `TopicModel`, реализующий EM-алгоритм в парадигме MapReduce:
- на шаге Map выполнять обрабатывать за раз целый partition, генерировать на выходе `numpy`-массив размера $|W|\times|T|$ счетчиков $n_{wt}$ и значение лог-правдоподобия, максимально использовать векторизацию операций;
- на шаге Reduce агрегировать полученные матрицы и значения лог-правдоподобия;
- на M-шаге нормировать счетчики и сохранять новую версию матрицы $\Phi$;
- реализовать возможность использования $\Phi$ в виде broadcast-переменной;
- обучение должно производиться с помощью метода `fit`;
- реализовать метод `print_perplexity`, печатающий список значений перплексии на конец каждой итерации обработки коллекции;
- реализовать метод `print_topics`, печатающий список из `num_tokens` наиболее вероятных слов в каждой теме;
2. Провести ряд экспериментов с реализованной моделью на данных `'vw.wiki-en-20K.txt'`:
- обучить модель с числом тем `num_topics` = 10, 20, 50, оценить время работы при `num_document_passes=5` и `num_collection_passes=10` в случае использования бродкастинга и без него, сделать выводы; перед запуском обучения нужно убедиться, что данные разбиты на достаточное число партиций и что среди них нет вырожденных (существенно меньших по объёму, чем прочие), информацию о разбиении нужно привести в отчете;
- при `num_topics=20` и `num_collection_passes=10` попробовать различные значения `num_document_passes` = 1, 2, 5, 10, оценить скорость работы и сравнить значения перплексии и интерпретируемость тем по топ-словам, сделать выводы;
3. Добавить в EM-алгоритм простейшую регуляризацию а-ля LDA (сглаживание/разреживание константой `beta`):
- попробовать разреживание с `beta`= 0.0, -0.1, -1.0 при `num_topics=20`, `num_collection_passes=10` и `num_document_passes=5`, сравнить значения перплексии и разреженность итоговой матрицы $\Phi$, сделать выводы;
- подобрать коэффициент `beta`, дающий разреженность итоговой $\Phi$ выше 95 процентов;
- заменить в реализации структуру для хранения счётчиков $n_{wt}$ и матрицы $\Phi$ с `numpy`-массива на любую другую из `scipy` или библиотеки Spark, подходяющую для хранения разреженных матриц;
- повторить эксперимент из п. 2.1, сравнить скорость работы с плотным вариантом хранения при использовании сильного разреживания (> 95 процентов).

 Все результаты экспериментов должны оформляться в виде аккуратных таблиц со значениями и списков с выводами, все должно читаться и пониматься легко без кода.

### Примечания:

1. Чтение данных проводится прилагаемой к заданию функцией `read_txt`, которая возвращает данные в нужном формате, векторизатор и значение `nnz`, необходимое для подсчета перплексии. При чтении коллекции фильтруйте слишком частый (более 30К вхождений) и слишком редкие (меньше 10 вхождений) слова, а также слова из менее чем 3 символов.
2. Для реализации п. 1.1 может быть полезен метод `mapPartitions`. 
3. Для работы с бродкастингом удобно завести внешний класс, создающий и обновляющий эту переменную на каждой итерации обработки коллекции, и внутренний, получающий её на каждой итерации как внешний параметр и выполняющий E-шаг. В любой случае, никаких глобальных переменных в коде быть не должно и класс `TopicModel` может получать снаружи только те параметры, которые указаны в примере использования.

### Как сдать задание

1. Для каждой задачи (в данном случае она всего 1) для вас создана ветка, в которую нужно заливать решение.
2. Для сдачи задания нужно до deadline сделать pull request в master. В нем должен быть Jupyter-ноутбук, содержащий весь код решения и отчёт с результатми экспериментов и выводами.
3. После deadline проверяющий оставляет комментарии и ставит *текущую оценку*. Если решение вцелом работает правильно и соответсвует требованиям, его можно исправлять до экзамена и повысить таким образом оценку.
